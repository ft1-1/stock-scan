name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-benchmark pytest-mock aioresponses
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ \
          --verbose \
          --cov=src \
          --cov-report=term-missing \
          --cov-report=xml \
          --cov-fail-under=80 \
          --maxfail=10 \
          -n auto
      env:
        ENVIRONMENT: testing
        EODHD_API_KEY: ${{ secrets.EODHD_API_KEY_TEST }}
        MARKETDATA_API_KEY: ${{ secrets.MARKETDATA_API_KEY_TEST }}
        CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY_TEST }}
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-asyncio aioresponses
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ \
          --verbose \
          --maxfail=5 \
          -m "integration and not slow" \
          --tb=short
      env:
        ENVIRONMENT: testing
        EODHD_API_KEY: ${{ secrets.EODHD_API_KEY_TEST }}
        MARKETDATA_API_KEY: ${{ secrets.MARKETDATA_API_KEY_TEST }}
        CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY_TEST }}

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[performance]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark psutil
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ \
          --verbose \
          --benchmark-only \
          --benchmark-sort=mean \
          --benchmark-json=benchmark_results.json \
          -m "performance and not slow"
      env:
        ENVIRONMENT: testing
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: Python Benchmark
        tool: 'pytest'
        output-file-path: benchmark_results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true

  security-tests:
    name: Security & Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety mypy black isort flake8
        pip install -r requirements.txt
    
    - name: Run security scan with Bandit
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Run safety check
      run: safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Run type checking with MyPy
      run: mypy src/ --ignore-missing-imports --json-report mypy-report
      continue-on-error: true
    
    - name: Check code formatting
      run: |
        black --check src/ tests/
        isort --check-only src/ tests/
        flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          mypy-report/

  load-tests:
    name: Load Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[load-test]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil
    
    - name: Run load tests
      run: |
        pytest tests/performance/ \
          --verbose \
          -m "performance and slow" \
          --tb=short \
          --maxfail=3
      env:
        ENVIRONMENT: testing
        LOAD_TEST_SCALE: medium
    
    - name: Generate load test report
      run: |
        python -c "
        import sys
        sys.path.insert(0, 'tests/performance')
        from test_load_performance import LoadTestSuite
        suite = LoadTestSuite()
        print(suite.generate_load_test_report())
        " > load_test_report.txt
    
    - name: Upload load test report
      uses: actions/upload-artifact@v3
      with:
        name: load-test-report
        path: load_test_report.txt

  memory-tests:
    name: Memory & Leak Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[memory]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil memory-profiler
    
    - name: Run memory tests
      run: |
        pytest tests/performance/test_memory_performance.py \
          --verbose \
          -m "performance" \
          --tb=short
      env:
        ENVIRONMENT: testing
    
    - name: Generate memory report
      run: |
        python -c "
        import sys
        sys.path.insert(0, 'tests/performance')
        from test_memory_performance import MemoryTestSuite
        suite = MemoryTestSuite()
        print(suite.generate_memory_report())
        " > memory_report.txt
    
    - name: Upload memory report
      uses: actions/upload-artifact@v3
      with:
        name: memory-report
        path: memory_report.txt

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests]
    if: always()
    
    steps:
    - name: Download all reports
      uses: actions/download-artifact@v3
    
    - name: Generate test summary
      run: |
        echo "# Test Execution Summary" > test_summary.md
        echo "" >> test_summary.md
        echo "## Test Results" >> test_summary.md
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> test_summary.md
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> test_summary.md
        echo "- Security Tests: ${{ needs.security-tests.result }}" >> test_summary.md
        echo "" >> test_summary.md
        echo "## Coverage & Quality" >> test_summary.md
        echo "- Code coverage reports available in artifacts" >> test_summary.md
        echo "- Security scan results available in artifacts" >> test_summary.md
        echo "" >> test_summary.md
        echo "Generated on: $(date)" >> test_summary.md
    
    - name: Comment PR with summary
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('test_summary.md', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

  notify-slack:
    name: Notify Slack
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests]
    if: always() && (github.event_name == 'schedule' || github.ref == 'refs/heads/main')
    
    steps:
    - name: Notify Slack on failure
      if: contains(needs.*.result, 'failure')
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#alerts'
        text: 'Options Screening System: Test failures detected in CI pipeline'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    
    - name: Notify Slack on success
      if: github.event_name == 'schedule' && !contains(needs.*.result, 'failure')
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#dev-updates'
        text: 'Options Screening System: All scheduled tests passed successfully'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}